{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e012de",
   "metadata": {},
   "source": [
    "# 2. RAG Question Answering for Car Intelligent Assistant\n",
    "\n",
    "Having a vector dataset generated with step 1. This code shows how to perform questions and get answers augmented with context from the dataset. This is called \"Retrieval Augmented Generation\" or RAG.\n",
    "\n",
    "Authors:\n",
    "- Luis Bernardo Hernandez Salinas\n",
    "- Juan R. Terven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc9ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from getpass import getpass\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947d552",
   "metadata": {},
   "source": [
    "## Set API, embedding and vector database path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee5cfd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model open-mixtral-8x22b\n",
      "Using vector database vector_database_chspark_1536\n"
     ]
    }
   ],
   "source": [
    "# List of models tested:\n",
    "# gpt-3.5-turbo\n",
    "# gpt-4-turbo\n",
    "# gpt-4o\n",
    "# claude-3-haiku-20240307\n",
    "# claude-3-sonnet-20240229\n",
    "# claude-3-opus-20240229\n",
    "# open-mistral-7b\n",
    "# open-mixtral-8x7b\n",
    "# open-mixtral-8x22b\n",
    "\n",
    "# Model to use\n",
    "llm_name = \"open-mixtral-8x22b\"\n",
    "\n",
    "embedding_dimensions = 1536 #1536  # 3072\n",
    "\n",
    "# API key \n",
    "if \"gpt\" in llm_name:\n",
    "    client = os.environ['OPENAI_API_KEY']\n",
    "elif \"claude\" in llm_name:\n",
    "    client = os.environ['ANTHROPIC_API_KEY']\n",
    "elif \"mistral\" in llm_name or \"mixtral\" in llm_name:\n",
    "    client = os.environ['MISTRAL_API_KEY']\n",
    "else:\n",
    "    print(\"INVALID MODEL!\")\n",
    "    \n",
    "print(f\"Using model {llm_name}\")\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Vector dataset\n",
    "vectordb_directory = f'vector_database_chspark_{embedding_dimensions}'\n",
    "print(f\"Using vector database {vectordb_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b27ad7",
   "metadata": {},
   "source": [
    "## Load Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f77e9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 243 collections from vector database\n"
     ]
    }
   ],
   "source": [
    "# Create chroma db from existing vectordb_directory\n",
    "vectordb = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=vectordb_directory\n",
    ")\n",
    "\n",
    "print(f\"Load {vectordb._collection.count()} collections from vector database\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d52b42",
   "metadata": {},
   "source": [
    "## Model instruction for Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea04c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\\\n",
    "You are an assistant and you have to answer the questions that are asked of you. \n",
    "If the question is about the vehicle, use the provided car manual information to answer the question at the end. \n",
    "If you don’t know the answer even with the car manual provided say \"I am sorry, I did not find the answer in the car manual\"\n",
    "Don’t try to make up an answer.\n",
    "Respond in the most attentive way possible. Use a maximum of three sentences. \n",
    "Keep the answer as concise as possible. \n",
    "Always say “Do you have any other questions?” at the end of the answer!\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0514089",
   "metadata": {},
   "source": [
    "### Set LLM and RAG object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65747e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<httpx.Client object at 0x000001BE27019ED0> async_client=<httpx.AsyncClient object at 0x000001BE2846BA50> mistral_api_key=SecretStr('**********') model='open-mixtral-8x22b' temperature=0.0\n"
     ]
    }
   ],
   "source": [
    "# create prompt template object\n",
    "qa_chain_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "if \"gpt\" in llm_name:\n",
    "    llm = ChatOpenAI(model_name=llm_name, temperature=0) \n",
    "elif \"claude\" in llm_name:    \n",
    "    llm = ChatAnthropic(model_name=llm_name, api_key=client, temperature=0)\n",
    "elif \"mistral\" in llm_name or \"mixtral\" in llm_name:\n",
    "    llm = ChatMistralAI(model=llm_name, api_key=client, temperature=0)\n",
    "\n",
    "print(llm)\n",
    "\n",
    "# QA RAG object\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_chain_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5090e",
   "metadata": {},
   "source": [
    "## Let's try RAG with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2074a44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, a pregnant woman should use the seatbelts. The lap belt should be positioned below the belly, across the hips, and the shoulder belt should be placed across the chest, between the breasts, and away from the neck. This is to ensure safety during travel. Do you have any other questions?\n"
     ]
    }
   ],
   "source": [
    "#query = \"what is the recommended fuel for this vehicle?\"\n",
    "query = \"should a pregnant woman use the seatbelts?\"\n",
    "\n",
    "# Run RAG chain: \n",
    "# 1. Take the query and add it to the prompt\n",
    "# 2. Get query embedding\n",
    "# 3. Retrieve the most relevant documents based on embedding similarity\n",
    "# 4. Augment the prompt with the retrieved documents\n",
    "# 5. Send prompt to LLM\n",
    "# 6. Get model response\n",
    "model_response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(model_response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48ea44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saci",
   "language": "python",
   "name": "saci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
